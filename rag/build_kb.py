import argparse
import os
import json
import numpy as np
import copy
from config import config
from core.llm_client import get_embedding
from core.vectordb import VectorDB

def build_database():
    print("=== æ­£åœ¨å¯åŠ¨ RAG æ•°æ®åº“ç‰©ç†éš”ç¦»å¼æ„å»º ===")
    
    for entity_type in config.ENTITY_TYPES:
        db_key = config.ENTITY_MAP.get(entity_type, entity_type)
        print(f"\næ£€æŸ¥å®ä½“ç±»å‹: {entity_type} ...")
        db = VectorDB(db_name=db_key)
        
        # 1. ç‰©ç†å›æ»šä¿æŠ¤ï¼šå¦‚æœæ•°æ®é‡å¼‚å¸¸ï¼Œå›æ»šåˆ° 6702 æ¡å®‰å…¨ç‚¹
        if entity_type == "expert" and len(db.metadata) > 6702:
            print(f"  [Safety] æ­£åœ¨ç‰©ç†å›æ»šè‡³ 6702 æ¡å®‰å…¨ç‚¹...")
            import faiss
            safe_vectors = [db.index.reconstruct(i) for i in range(6702)]
            new_index = faiss.IndexFlatIP(config.EMBEDDING_DIMENSION)
            new_index.add(np.array(safe_vectors).astype('float32'))
            db.index = new_index
            db.metadata = db.metadata[:6702]
            db.save()

        # 2. æ„å»ºæŸ¥é‡é›†åˆ
        existing_titles = set()
        for meta in db.metadata:
            orig = meta.get("original_data", {})
            t = orig.get("title") or orig.get("data", {}).get("title")
            if t: existing_titles.add(str(t).strip())

        # 3. ç‰©ç†æ‰«ææ–‡ä»¶å¤¹è¿›è¡Œè¡¥å½•
        folder_path = os.path.join(config.RAW_DATA_ROOT, entity_type)
        if not os.path.exists(folder_path): continue
        
        files = [f for f in os.listdir(folder_path) if f.endswith('.json')]
        success_count = 0

        for file_name in files:
            file_path = os.path.join(folder_path, file_name)
            expert_name = file_name.replace(".json", "").strip()

            if expert_name in existing_titles:
                continue

            try:
                # 4. å¼ºåˆ¶ç‰©ç†é‡è¯»ï¼šç¡®ä¿è¯»å–çš„å†…å®¹ä¸æ–‡ä»¶åå®Œå…¨åŒ¹é…
                with open(file_path, 'r', encoding='utf-8') as f:
                    raw_data = json.load(f)
                    # é‡æ–°è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç”¨äº Embedding
                    text_for_embedding = json.dumps(raw_data, ensure_ascii=False)

                # 5. é•¿åº¦æˆªæ–­
                if len(text_for_embedding) > 30000:
                    text_for_embedding = text_for_embedding[:30000]

                print(f"  [æ–°å…¥åº“] {expert_name}...")
                vector = get_embedding(text_for_embedding)
                
                if vector:
                    # å¼ºè¡Œæ ¡å‡†å†…éƒ¨æ ‡é¢˜
                    if "data" in raw_data: raw_data["data"]["title"] = expert_name
                    raw_data["title"] = expert_name
                    
                    db.add_item(
                        text=text_for_embedding,
                        vector=vector,
                        original_data=copy.deepcopy(raw_data)
                    )
                    success_count += 1
            except Exception as e:
                print(f"  âŒ å¤„ç† {file_name} å¤±è´¥: {e}")

        if success_count > 0:
            db.save()
            print(f"  âœ… {entity_type} å¤„ç†å®Œæˆï¼Œæ–°å¢ {success_count} æ¡ã€‚")

def inspect_expert(name):
    """
    å¢å¼ºç‰ˆè°ƒè¯•å‡½æ•°ï¼šå±•ç¤ºå‘é‡ç‰¹å¾å¹¶ç²¾ç®€æ–‡æœ¬è¾“å‡º
    """
    from core.vectordb import VectorDB
    import re
    
    db = VectorDB(db_name="expert")
    vector, full_text = db.get_vector_by_name(name)
    
    if vector is not None:
        print(f"\n" + "â˜…" * 60)
        print(f"ã€ æ£€ç´¢é”® ã€‘: {name}")
        print("-" * 60)
        
        # 1. å‘é‡å‰ 20 ä½é¢„è§ˆ (æ ¼å¼åŒ–ä¸ºå°æ•°ç‚¹å 4 ä½)
        v_preview = [f"{x:.8f}" for x in vector[:20]]
        print(f"ã€ å‘é‡é¢„è§ˆ (å‰20ä½) ã€‘:\n{v_preview}")
        print("-" * 60)
        
        # 2. å°è¯•æå–å…ƒæ•°æ®æ ‡é¢˜
        title_match = re.search(r'"title":\s*"([^"]+)"', str(full_text))
        real_title = title_match.group(1) if title_match else "Unknown"
        print(f"ã€ å®é™…æ ‡é¢˜ ã€‘: {real_title}")
        
        # 3. ç²¾ç®€æ–‡æœ¬é¢„è§ˆ (ä»…å±•ç¤ºå‰ 150 ä¸ªå­—ç¬¦ï¼Œæ›´æ˜“è¯»)
        clean_text = str(full_text)[:150].replace('\n', ' ').strip()
        print(f"ã€ å†…å®¹é¢„è§ˆ ã€‘: {clean_text}...")
        
        # 4. æœ€ç»ˆçŠ¶æ€åˆ¤å®š
        if real_title.strip() == name.strip():
            print("\nâœ¨ çŠ¶æ€æ£€æŸ¥ï¼š[ æ­£å¸¸ ] - ç´¢å¼•ä¸ç‰©ç†å†…å®¹ä¸¥æ ¼ä¸€è‡´")
        else:
            print("\nğŸš¨ çŠ¶æ€æ£€æŸ¥ï¼š[ å¼‚å¸¸ ] - å‘ç°èº«ä»½é”™ä½ï¼")
            
        print("â˜…" * 60 + "\n")
    else:
        print(f"\nâŒ æ£€ç´¢å¤±è´¥ï¼šåº“ä¸­æœªæ‰¾åˆ°ä¸“å®¶ ã€{name}ã€‘\n")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--inspect', type=str)
    args = parser.parse_args()

    if args.inspect:
        inspect_expert(args.inspect)
    else:
        build_database()
